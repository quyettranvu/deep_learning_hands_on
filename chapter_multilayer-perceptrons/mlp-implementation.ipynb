{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quyettranvu/deep_learning_hands_on/blob/main/chapter_multilayer-perceptrons/mlp-implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0576b09f",
      "metadata": {
        "id": "0576b09f"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "9fd76644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd76644",
        "outputId": "dd7ae4ea-7215-4c49-b4a7-30758d408391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: d2l==1.0.3 in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy<2,>=1.26 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter) (7.7.1)\n",
            "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.12/dist-packages (from jupyter) (4.4.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (5.9.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->jupyter) (4.5.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (3.0.16)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (1.3.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->jupyter) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->jupyter) (4.25.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (0.28.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (2.14.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (4.11.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (4.15.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter) (25.1.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.23)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.8)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.4.0)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (2.0.5)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (2.3.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (2.28.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.5.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "d2l OK, numpy: 1.26.4\n"
          ]
        }
      ],
      "source": [
        "# keep pip up to date\n",
        "%pip install -U pip\n",
        "\n",
        "# install d2l but skip its (too strict) dependencies\n",
        "%pip install d2l==1.0.3 --no-deps\n",
        "\n",
        "# install dependencies compatible with Python 3.12\n",
        "# NumPy >= 1.26 has Py3.12 wheels\n",
        "%pip install \"numpy>=1.26,<2\" matplotlib pandas jupyter\n",
        "\n",
        "# Choose the right index for your runtime (CPU vs CUDA). Example for CUDA 12.4:\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "# Or CPU-only:\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "import d2l, numpy as np\n",
        "print(\"d2l OK, numpy:\", np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4fd052",
      "metadata": {
        "origin_pos": 1,
        "id": "2b4fd052"
      },
      "source": [
        "# Implementation of Multilayer Perceptrons\n",
        ":label:`sec_mlp-implementation`\n",
        "\n",
        "Multilayer perceptrons (MLPs) are not much more complex to implement than simple linear models. The key conceptual\n",
        "difference is that we now concatenate multiple layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "4972f9b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:59:39.988243Z",
          "iopub.status.busy": "2023-08-18T19:59:39.987704Z",
          "iopub.status.idle": "2023-08-18T19:59:42.965665Z",
          "shell.execute_reply": "2023-08-18T19:59:42.964666Z"
        },
        "origin_pos": 4,
        "tab": [
          "tensorflow"
        ],
        "id": "4972f9b1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211d3e3d",
      "metadata": {
        "origin_pos": 6,
        "id": "211d3e3d"
      },
      "source": [
        "## Implementation from Scratch\n",
        "\n",
        "Let's begin again by implementing such a network from scratch.\n",
        "\n",
        "### Initializing Model Parameters\n",
        "\n",
        "Recall that Fashion-MNIST contains 10 classes,\n",
        "and that each image consists of a $28 \\times 28 = 784$\n",
        "grid of grayscale pixel values.\n",
        "As before we will disregard the spatial structure\n",
        "among the pixels for now,\n",
        "so we can think of this as a classification dataset\n",
        "with 784 input features and 10 classes.\n",
        "To begin, we will [**implement an MLP\n",
        "with one hidden layer and 256 hidden units.**]\n",
        "Both the number of layers and their width are adjustable\n",
        "(they are considered hyperparameters).\n",
        "Typically, we choose the layer widths to be divisible by larger powers of 2.\n",
        "This is computationally efficient due to the way\n",
        "memory is allocated and addressed in hardware.\n",
        "\n",
        "Again, we will represent our parameters with several tensors.\n",
        "Note that *for every layer*, we must keep track of\n",
        "one weight matrix and one bias vector.\n",
        "As always, we allocate memory\n",
        "for the gradients of the loss with respect to these parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84bd1950",
      "metadata": {
        "origin_pos": 9,
        "tab": [
          "tensorflow"
        ],
        "id": "84bd1950"
      },
      "source": [
        "In the code below we use `tf.Variable`\n",
        "to define the model parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "4d2c8a35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:59:42.969805Z",
          "iopub.status.busy": "2023-08-18T19:59:42.969080Z",
          "iopub.status.idle": "2023-08-18T19:59:42.974775Z",
          "shell.execute_reply": "2023-08-18T19:59:42.973940Z"
        },
        "origin_pos": 13,
        "tab": [
          "tensorflow"
        ],
        "id": "4d2c8a35"
      },
      "outputs": [],
      "source": [
        "class MLPScratch(d2l.Classifier):\n",
        "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.W1 = tf.Variable(\n",
        "            tf.random.normal((num_inputs, num_hiddens)) * sigma)\n",
        "        self.b1 = tf.Variable(tf.zeros(num_hiddens))\n",
        "        self.W2 = tf.Variable(\n",
        "            tf.random.normal((num_hiddens, num_outputs)) * sigma)\n",
        "        self.b2 = tf.Variable(tf.zeros(num_outputs))\n",
        "        self.loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    def evaluate_accuracy(self, y_hat, y):\n",
        "        \"\"\"Compute accuracy for a multiclass classification problem.\"\"\"\n",
        "        if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "            y_hat = tf.argmax(y_hat, axis=1)\n",
        "        cmp = tf.cast(y_hat, y.dtype) == y\n",
        "        return tf.reduce_sum(tf.cast(cmp, y.dtype)) / len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188648fa",
      "metadata": {
        "origin_pos": 15,
        "id": "188648fa"
      },
      "source": [
        "### Model\n",
        "\n",
        "To make sure we know how everything works,\n",
        "we will [**implement the ReLU activation**] ourselves\n",
        "rather than invoking the built-in `relu` function directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "87f1e312",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:59:42.977992Z",
          "iopub.status.busy": "2023-08-18T19:59:42.977469Z",
          "iopub.status.idle": "2023-08-18T19:59:42.981226Z",
          "shell.execute_reply": "2023-08-18T19:59:42.980438Z"
        },
        "origin_pos": 18,
        "tab": [
          "tensorflow"
        ],
        "id": "87f1e312"
      },
      "outputs": [],
      "source": [
        "def relu(X):\n",
        "    return tf.math.maximum(X, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd3df8e",
      "metadata": {
        "origin_pos": 20,
        "id": "1bd3df8e"
      },
      "source": [
        "Since we are disregarding spatial structure,\n",
        "we `reshape` each two-dimensional image into\n",
        "a flat vector of length  `num_inputs`.\n",
        "Finally, we (**implement our model**)\n",
        "with just a few lines of code. Since we use the framework built-in autograd this is all that it takes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "fc556ba9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:59:42.984353Z",
          "iopub.status.busy": "2023-08-18T19:59:42.983834Z",
          "iopub.status.idle": "2023-08-18T19:59:42.988246Z",
          "shell.execute_reply": "2023-08-18T19:59:42.987456Z"
        },
        "origin_pos": 21,
        "tab": [
          "tensorflow"
        ],
        "id": "fc556ba9"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(MLPScratch)\n",
        "def forward(self, X):\n",
        "    X = tf.reshape(X, (-1, self.num_inputs))\n",
        "    H = relu(tf.matmul(X, self.W1) + self.b1)\n",
        "    return tf.matmul(H, self.W2) + self.b2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388c79e6",
      "metadata": {
        "origin_pos": 22,
        "id": "388c79e6"
      },
      "source": [
        "### Training\n",
        "\n",
        "Fortunately, [**the training loop for MLPs\n",
        "is exactly the same as for softmax regression.**] We define the model, data, and trainer, then finally invoke the `fit` method on model and data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "1fff9a8c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:59:42.991345Z",
          "iopub.status.busy": "2023-08-18T19:59:42.990832Z",
          "iopub.status.idle": "2023-08-18T20:00:34.763804Z",
          "shell.execute_reply": "2023-08-18T20:00:34.762935Z"
        },
        "origin_pos": 23,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "1fff9a8c",
        "outputId": "cbaeca9c-1941-4f18-b76e-a5a9151d0c08"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.float32' object has no attribute 'numpy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2224032715.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mfit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;34m\"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1889772355.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, key, value, train)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_val_batches\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_valid_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         self.board.draw(x, d2l.numpy(value), (\n\u001b[0m\u001b[1;32m    203\u001b[0m             'train_' if train else 'val_') + key, every_n=int(n))\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0msigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[0mexpand_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m \u001b[0mrepeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0mbatch_matmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute 'numpy'"
          ]
        }
      ],
      "source": [
        "model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)\n",
        "data = d2l.FashionMNIST(batch_size=256)\n",
        "trainer = d2l.Trainer(max_epochs=10)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26978243"
      },
      "source": [
        "@d2l.add_to_class(d2l.Classifier)\n",
        "def evaluate_step(self, batch):\n",
        "    X, y = batch[:-1][0], batch[-1]\n",
        "    y_hat = self(X, training=False)\n",
        "    l = self.loss(tf.one_hot(y, depth=y_hat.shape[-1]), y_hat)\n",
        "    self.plot('loss', l, train=False)\n",
        "    self.plot('acc', self.evaluate_accuracy(y_hat, y), train=False)\n",
        "    return l"
      ],
      "id": "26978243",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff7dabd5"
      },
      "source": [
        "@d2l.add_to_class(MLPScratch)\n",
        "def backward(self, loss, tape):\n",
        "    params = [self.W1, self.b1, self.W2, self.b2]\n",
        "    grads = tape.gradient(loss, params)\n",
        "    self.trainer.optim.apply_gradients(zip(grads, params))"
      ],
      "id": "ff7dabd5",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ba044f"
      },
      "source": [
        "@d2l.add_to_class(d2l.Classifier)\n",
        "def training_step(self, batch):\n",
        "    X, y = batch[:-1][0], batch[-1]\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = self(X, training=True)\n",
        "\n",
        "        # Convert y to one-hot encoding\n",
        "        y_one_hot = tf.one_hot(y, depth=y_hat.shape[-1])\n",
        "\n",
        "        l = self.loss(y_one_hot, y_hat)\n",
        "    self.backward(l, tape)\n",
        "    self.plot('loss', l.numpy(), train=True)\n",
        "    self.plot('acc', self.evaluate_accuracy(y_hat, y), train=True)\n",
        "    return l"
      ],
      "id": "f9ba044f",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d9ae0580",
      "metadata": {
        "origin_pos": 24,
        "id": "d9ae0580"
      },
      "source": [
        "## Concise Implementation\n",
        "\n",
        "As you might expect, by relying on the high-level APIs, we can implement MLPs even more concisely.\n",
        "\n",
        "### Model\n",
        "\n",
        "Compared with our concise implementation\n",
        "of softmax regression implementation\n",
        "(:numref:`sec_softmax_concise`),\n",
        "the only difference is that we add\n",
        "*two* fully connected layers where we previously added only *one*.\n",
        "The first is [**the hidden layer**],\n",
        "the second is the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c0eb4479",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:00:34.767699Z",
          "iopub.status.busy": "2023-08-18T20:00:34.767006Z",
          "iopub.status.idle": "2023-08-18T20:00:34.772093Z",
          "shell.execute_reply": "2023-08-18T20:00:34.771303Z"
        },
        "origin_pos": 27,
        "tab": [
          "tensorflow"
        ],
        "id": "c0eb4479"
      },
      "outputs": [],
      "source": [
        "class MLP(d2l.Classifier):\n",
        "    def __init__(self, num_outputs, num_hiddens, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(num_hiddens, activation='relu'),\n",
        "            tf.keras.layers.Dense(num_outputs)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340a7eed",
      "metadata": {
        "origin_pos": 29,
        "id": "340a7eed"
      },
      "source": [
        "Previously, we defined `forward` methods for models to transform input using the model parameters.\n",
        "These operations are essentially a pipeline:\n",
        "you take an input and\n",
        "apply a transformation (e.g.,\n",
        "matrix multiplication with weights followed by bias addition),\n",
        "then repetitively use the output of the current transformation as\n",
        "input to the next transformation.\n",
        "However, you may have noticed that\n",
        "no `forward` method is defined here.\n",
        "In fact, `MLP` inherits the `forward` method from the `Module` class (:numref:`subsec_oo-design-models`) to\n",
        "simply invoke `self.net(X)` (`X` is input),\n",
        "which is now defined as a sequence of transformations\n",
        "via the `Sequential` class.\n",
        "The `Sequential` class abstracts the forward process\n",
        "enabling us to focus on the transformations.\n",
        "We will further discuss how the `Sequential` class works in :numref:`subsec_model-construction-sequential`.\n",
        "\n",
        "\n",
        "### Training\n",
        "\n",
        "[**The training loop**] is exactly the same\n",
        "as when we implemented softmax regression.\n",
        "This modularity enables us to separate\n",
        "matters concerning the model architecture\n",
        "from orthogonal considerations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "18e5a172",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:00:34.775446Z",
          "iopub.status.busy": "2023-08-18T20:00:34.774941Z",
          "iopub.status.idle": "2023-08-18T20:01:27.256977Z",
          "shell.execute_reply": "2023-08-18T20:01:27.256143Z"
        },
        "origin_pos": 30,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "18e5a172",
        "outputId": "176e4c08-21b1-41cc-d35e-1cdcae8cfde4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3036746132.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mfit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;34m\"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/d2l/tensorflow.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m             'train_' if train else 'val_') + key, every_n=int(n))\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ],
      "source": [
        "model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\n",
        "trainer.fit(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c69640c2",
      "metadata": {
        "origin_pos": 31,
        "id": "c69640c2"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Now that we have more practice in designing deep networks, the step from a single to multiple layers of deep networks does not pose such a significant challenge any longer. In particular, we can reuse the training algorithm and data loader. Note, though, that implementing MLPs from scratch is nonetheless messy: naming and keeping track of the model parameters makes it difficult to extend models. For instance, imagine wanting to insert another layer between layers 42 and 43. This might now be layer 42b, unless we are willing to perform sequential renaming. Moreover, if we implement the network from scratch, it is much more difficult for the framework to perform meaningful performance optimizations.\n",
        "\n",
        "Nonetheless, you have now reached the state of the art of the late 1980s when fully connected deep networks were the method of choice for neural network modeling. Our next conceptual step will be to consider images. Before we do so, we need to review a number of statistical basics and details on how to compute models efficiently.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Change the number of hidden units `num_hiddens` and plot how its number affects the accuracy of the model. What is the best value of this hyperparameter?\n",
        "1. Try adding a hidden layer to see how it affects the results.\n",
        "1. Why is it a bad idea to insert a hidden layer with a single neuron? What could go wrong?\n",
        "1. How does changing the learning rate alter your results? With all other parameters fixed, which learning rate gives you the best results? How does this relate to the number of epochs?\n",
        "1. Let's optimize over all hyperparameters jointly, i.e., learning rate, number of epochs, number of hidden layers, and number of hidden units per layer.\n",
        "    1. What is the best result you can get by optimizing over all of them?\n",
        "    1. Why it is much more challenging to deal with multiple hyperparameters?\n",
        "    1. Describe an efficient strategy for optimizing over multiple parameters jointly.\n",
        "1. Compare the speed of the framework and the from-scratch implementation for a challenging problem. How does it change with the complexity of the network?\n",
        "1. Measure the speed of tensor--matrix multiplications for well-aligned and misaligned matrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and 1032.\n",
        "    1. How does this change between GPUs and CPUs?\n",
        "    1. Determine the memory bus width of your CPU and GPU.\n",
        "1. Try out different activation functions. Which one works best?\n",
        "1. Is there a difference between weight initializations of the network? Does it matter?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wk9wPnRtN7R7"
      },
      "id": "Wk9wPnRtN7R7"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np, time, itertools, math\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# --- Data ---\n",
        "(xtr, ytr), (xte, yte) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "xtr = (xtr.astype(\"float32\")/255.0).reshape(-1, 28*28)\n",
        "xte = (xte.astype(\"float32\")/255.0).reshape(-1, 28*28)\n",
        "\n",
        "def build_mlp(\n",
        "    input_dim=784,\n",
        "    num_classes=10,\n",
        "    hidden_layers=(256,),\n",
        "    activation=\"relu\",\n",
        "    kernel_init=\"he_normal\",   # \"glorot_uniform\" for tanh/sigmoid\n",
        "    use_bn=False,\n",
        "    dropout=0.0\n",
        "):\n",
        "    m = models.Sequential()\n",
        "    m.add(layers.Input(shape=(input_dim,)))\n",
        "    for h in hidden_layers:\n",
        "        m.add(layers.Dense(h, activation=None, kernel_initializer=kernel_init))\n",
        "        if use_bn:\n",
        "            m.add(layers.BatchNormalization())\n",
        "        m.add(layers.Activation(activation))\n",
        "        if dropout>0:\n",
        "            m.add(layers.Dropout(dropout))\n",
        "    m.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "    return m\n",
        "\n",
        "def train_eval(\n",
        "    hidden_layers=(256,),\n",
        "    lr=1e-2,\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    activation=\"relu\",\n",
        "    kernel_init=\"he_normal\",\n",
        "    use_bn=False,\n",
        "    dropout=0.0,\n",
        "    verbose=0\n",
        "):\n",
        "    m = build_mlp(\n",
        "        hidden_layers=hidden_layers, activation=activation,\n",
        "        kernel_init=kernel_init, use_bn=use_bn, dropout=dropout\n",
        "    )\n",
        "    m.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "    hist = m.fit(xtr, ytr, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "    test_loss, test_acc = m.evaluate(xte, yte, verbose=0)\n",
        "    return test_acc, hist.history"
      ],
      "metadata": {
        "id": "bAJRSyO5N7mv"
      },
      "id": "bAJRSyO5N7mv",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "99f94f34",
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "tensorflow"
        ],
        "id": "99f94f34"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/227)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vary the number of hidden inputs\n",
        "units_grid = [32, 64, 128, 256, 512, 1024]\n",
        "results_grid = {}\n",
        "\n",
        "# kernel_init: chn cch t gi tr ban u cho trng s  m hnh hc hiu qu hn, use_bn: s dng BatchNorm  tng  hi t, gim overfitting, verbose = 0: khng in g\n",
        "for u in units_grid:\n",
        "  acc, _ = train_eval(hidden_layers=(u,), lr=0.05, epochs=10, activation=\"relu\", kernel_init=\"he_normal\", use_bn = True, dropout = 0.0, verbose = 0)\n",
        "  results_grid[u] = acc\n",
        "\n",
        "print(results_grid) # dataset = 128, 256 hi t tt hn, khng b qu khp"
      ],
      "metadata": {
        "id": "bv2OklPUOLor",
        "outputId": "f26a0be1-65c0-4654-997e-ac5260480e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bv2OklPUOLor",
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{32: 0.8525999784469604, 64: 0.8680999875068665, 128: 0.8707000017166138, 256: 0.8751999735832214, 512: 0.8575000166893005, 1024: 0.8436999917030334}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_grid = 256\n",
        "acc_1, _ = train_eval(hidden_layers=(256,), lr=0.05, epochs=10, activation=\"relu\", kernel_init=\"he_normal\", use_bn = True, dropout = 0.0, verbose = 0)\n",
        "acc_2, _ = train_eval(hidden_layers=(256,128), lr=0.05, epochs=10, use_bn=True)\n",
        "print (\"Accuracy 1: {:.4f}, Accuracy 2: {:.4f}\".format(acc_1, acc_2))"
      ],
      "metadata": {
        "id": "DnqcEZYOPoIf",
        "outputId": "399e9220-937d-441f-a6be-d6c9fc21b093",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DnqcEZYOPoIf",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 1: 0.8722, Accuracy 2: 0.8729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single hidden neuron creates a severe bottleneck: the network must compress the 784-dim input into one scalar before classifying 10 classes  thats an extreme information loss. It forces almost-linear decision boundaries after that bottleneck and typically tanks accuracy."
      ],
      "metadata": {
        "id": "c_x8uBHiQV8J"
      },
      "id": "c_x8uBHiQV8J"
    },
    {
      "cell_type": "code",
      "source": [
        "lrs = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
        "res_lr = {}\n",
        "for lr in lrs:\n",
        "    acc, _ = train_eval(hidden_layers=(256,), lr=lr, epochs=10, use_bn=True, verbose=0)\n",
        "    res_lr[lr] = acc\n",
        "res_lr"
      ],
      "metadata": {
        "id": "VFKxWGhKQ2Ce",
        "outputId": "fded0a4f-8b24-47ea-e873-98377128236c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VFKxWGhKQ2Ce",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.001: 0.8418999910354614,\n",
              " 0.01: 0.8673999905586243,\n",
              " 0.05: 0.8705000281333923,\n",
              " 0.1: 0.8705000281333923,\n",
              " 0.2: 0.8723999857902527}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "search_space = {\n",
        "    \"lr\": [0.005, 0.01, 0.02, 0.05, 0.1],\n",
        "    \"epochs\": [5, 10, 15],\n",
        "    \"layers\": [(128,), (256,), (512,), (256,128), (512,256)],\n",
        "    \"activation\": [\"relu\", \"gelu\", \"tanh\"],\n",
        "    \"kernel_init\": [\"he_normal\", \"glorot_uniform\"],\n",
        "    \"use_bn\": [False, True],\n",
        "    \"dropout\": [0.0, 0.2]\n",
        "}\n",
        "best = (-1, None)\n",
        "for i in range(20):  # increase for a deeper search\n",
        "    cfg = {k: random.choice(v) for k,v in search_space.items()}\n",
        "    acc, _ = train_eval(hidden_layers=cfg[\"layers\"], lr=cfg[\"lr\"], epochs=cfg[\"epochs\"],\n",
        "                        activation=cfg[\"activation\"], kernel_init=cfg[\"kernel_init\"],\n",
        "                        use_bn=cfg[\"use_bn\"], dropout=cfg[\"dropout\"], verbose=0)\n",
        "    if acc > best[0]:\n",
        "        best = (acc, cfg)\n",
        "best"
      ],
      "metadata": {
        "id": "kDYH7hQSQ_x8",
        "outputId": "c4f18d94-ff39-4ad0-9290-2a0e673f30ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kDYH7hQSQ_x8",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8769000172615051,\n",
              " {'lr': 0.02,\n",
              "  'epochs': 15,\n",
              "  'layers': (256, 128),\n",
              "  'activation': 'relu',\n",
              "  'kernel_init': 'glorot_uniform',\n",
              "  'use_bn': False,\n",
              "  'dropout': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras/TF use fused kernels, cuDNN/cuBLAS  much faster than manual NumPy/TensorFlow-eager loops. As network complexity grows (more layers/params/batches), the gap widens because frameworks better utilize vectorization and GPUs."
      ],
      "metadata": {
        "id": "Xo3INoe2RKB_"
      },
      "id": "Xo3INoe2RKB_"
    },
    {
      "cell_type": "code",
      "source": [
        "def bench_matmul(n, device = None, iters = 50, warmup = 10):\n",
        "  # define A,B\n",
        "  A = tf.random.normal((n, n))\n",
        "  B = tf.random.normal((n, n))\n",
        "\n",
        "  # device '/CPU:0' or '/GPU:0'\n",
        "  if device:\n",
        "    with tf.device(device):\n",
        "      for _ in range(warmup):\n",
        "        _ = tf.linalg.matmul(A, B)\n",
        "      start = time.time()\n",
        "      for _ in range(iters):\n",
        "        _ = tf.linalg.matmul(A, B)\n",
        "      tf.experimental.sync_devices()\n",
        "\n",
        "  # else\n",
        "  else:\n",
        "    for _ in range(warmup):\n",
        "      _ = tf.linalg.matmul(A, B)\n",
        "      start = time.time()\n",
        "    for _ in range(iters):\n",
        "      _ = tf.linalg.matmul(A, B)\n",
        "  current_time = time.time()\n",
        "  return (current_time - start) / iters\n",
        "\n",
        "\n",
        "sizes = [1024, 1025, 1026, 1028, 1032]\n",
        "cpu_times = {n: bench_matmul(n, device=\"/CPU:0\") for n in sizes}\n",
        "gpu_times = {}\n",
        "try:\n",
        "    tf.config.list_physical_devices('GPU')[0]\n",
        "    gpu_times = {n: bench_matmul(n, device=\"/GPU:0\") for n in sizes}\n",
        "except IndexError:\n",
        "    pass\n",
        "\n",
        "cpu_times, gpu_times\n"
      ],
      "metadata": {
        "id": "JashklRDRloD"
      },
      "id": "JashklRDRloD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU: optimized on large batch dense operations (matmul, conv,...), CPU: competitive on small models/batches to launch overheads.\n",
        "\n",
        "Memory bus width: hardware property (128-bit/256-bit/320-bit/384-bit for GPUs):estimate effective bandwidth by timing large tensor reads/writes, computing GB/s."
      ],
      "metadata": {
        "id": "5nwKrP9mTJSj"
      },
      "id": "5nwKrP9mTJSj"
    },
    {
      "cell_type": "code",
      "source": [
        "acts = [\"relu\", \"gelu\", \"tanh\", \"elu\", \"selu\"]\n",
        "act_res = {}\n",
        "for a in acts:\n",
        "    acc, _ = train_eval(hidden_layers=(256,128), lr=0.02, epochs=15,\n",
        "                        activation=a, kernel_init=\"glorot_uniform\", use_bn=False, verbose=0)\n",
        "    act_res[a] = acc\n",
        "act_res"
      ],
      "metadata": {
        "id": "RofGtmRxT5Nn",
        "outputId": "b84cd8bd-895c-4777-9d2e-ddd0e3ba26cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RofGtmRxT5Nn",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'relu': 0.883400022983551,\n",
              " 'gelu': 0.8830000162124634,\n",
              " 'tanh': 0.8813999891281128,\n",
              " 'elu': 0.8756999969482422,\n",
              " 'selu': 0.8723999857902527}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relu, GELU, Elu: s dng kernel init l He/Kaiming, He_normal, He_uniform\n",
        "tanh: dng xavier vi glorot cho kernel init v learning rate thp, SELU: dng Lecun normal no BN. Thc t phi c dropout nh trong cc case."
      ],
      "metadata": {
        "id": "M05-QIq7Ui7a"
      },
      "id": "M05-QIq7Ui7a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical tips / likely best settings (Fashion-MNIST, fast runs)\n",
        "\t\t1 hidden layer 256512 ReLU, He init, BN on, SGD+Momentum 0.9, LR0.05, epochs 1015 is a very solid baseline (~0.880.90+ test acc).\n",
        "\t\tAdd a second layer (e.g., 256,128) for a bit more accuracy if you can afford a few more epochs or add dropout 0.2."
      ],
      "metadata": {
        "id": "_qH6GL-uUgjD"
      },
      "id": "_qH6GL-uUgjD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}