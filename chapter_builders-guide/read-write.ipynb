{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b4cadb2",
      "metadata": {
        "origin_pos": 1,
        "id": "8b4cadb2"
      },
      "source": [
        "# File I/O\n",
        "\n",
        "So far we have discussed how to process data and how\n",
        "to build, train, and test deep learning models.\n",
        "However, at some point we will hopefully be happy enough\n",
        "with the learned models that we will want\n",
        "to save the results for later use in various contexts\n",
        "(perhaps even to make predictions in deployment).\n",
        "Additionally, when running a long training process,\n",
        "the best practice is to periodically save intermediate results (checkpointing)\n",
        "to ensure that we do not lose several days' worth of computation\n",
        "if we trip over the power cord of our server.\n",
        "Thus it is time to learn how to load and store\n",
        "both individual weight vectors and entire models.\n",
        "This section addresses both issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3dff9f92",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:02.401475Z",
          "iopub.status.busy": "2023-08-18T20:14:02.401201Z",
          "iopub.status.idle": "2023-08-18T20:14:04.212167Z",
          "shell.execute_reply": "2023-08-18T20:14:04.211057Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "3dff9f92"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d2d6c9",
      "metadata": {
        "origin_pos": 6,
        "id": "71d2d6c9"
      },
      "source": [
        "## (**Loading and Saving Tensors**)\n",
        "\n",
        "For individual tensors, we can directly\n",
        "invoke the `load` and `save` functions\n",
        "to read and write them respectively.\n",
        "Both functions require that we supply a name,\n",
        "and `save` requires as input the variable to be saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "41c45edb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.216976Z",
          "iopub.status.busy": "2023-08-18T20:14:04.216107Z",
          "iopub.status.idle": "2023-08-18T20:14:04.247988Z",
          "shell.execute_reply": "2023-08-18T20:14:04.246686Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "41c45edb"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(4)\n",
        "torch.save(x, 'x-file')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d4cdfe",
      "metadata": {
        "origin_pos": 11,
        "id": "e4d4cdfe"
      },
      "source": [
        "We can now read the data from the stored file back into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5cb11ed0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.253634Z",
          "iopub.status.busy": "2023-08-18T20:14:04.253018Z",
          "iopub.status.idle": "2023-08-18T20:14:04.262485Z",
          "shell.execute_reply": "2023-08-18T20:14:04.261617Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "5cb11ed0",
        "outputId": "697e0db9-2ca2-4d7f-aee7-9bd3d939bd38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x2 = torch.load('x-file')\n",
        "x2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431f40d8",
      "metadata": {
        "origin_pos": 16,
        "id": "431f40d8"
      },
      "source": [
        "We can [**store a list of tensors and read them back into memory.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "86dba6ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.267689Z",
          "iopub.status.busy": "2023-08-18T20:14:04.267013Z",
          "iopub.status.idle": "2023-08-18T20:14:04.275474Z",
          "shell.execute_reply": "2023-08-18T20:14:04.274471Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "86dba6ad",
        "outputId": "627b2e34-79d6-4a3d-9388-2d44e74ed355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "y = torch.zeros(4)\n",
        "torch.save([x, y],'x-files')\n",
        "x2, y2 = torch.load('x-files')\n",
        "(x2, y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e03e08f",
      "metadata": {
        "origin_pos": 21,
        "id": "7e03e08f"
      },
      "source": [
        "We can even [**write and read a dictionary that maps\n",
        "from strings to tensors.**]\n",
        "This is convenient when we want\n",
        "to read or write all the weights in a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1b8f14c8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.279967Z",
          "iopub.status.busy": "2023-08-18T20:14:04.279484Z",
          "iopub.status.idle": "2023-08-18T20:14:04.286799Z",
          "shell.execute_reply": "2023-08-18T20:14:04.286045Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "1b8f14c8",
        "outputId": "d4d91565-81e2-4d87-fa1b-e2e32269085e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "mydict = {'x': x, 'y': y}\n",
        "torch.save(mydict, 'mydict')\n",
        "mydict2 = torch.load('mydict')\n",
        "mydict2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f21d5a",
      "metadata": {
        "origin_pos": 26,
        "id": "a1f21d5a"
      },
      "source": [
        "## [**Loading and Saving Model Parameters**]\n",
        "\n",
        "Saving individual weight vectors (or other tensors) is useful,\n",
        "but it gets very tedious if we want to save\n",
        "(and later load) an entire model.\n",
        "After all, we might have hundreds of\n",
        "parameter groups sprinkled throughout.\n",
        "For this reason the deep learning framework provides built-in functionalities\n",
        "to load and save entire networks.\n",
        "An important detail to note is that this\n",
        "saves model *parameters* and not the entire model.\n",
        "For example, if we have a 3-layer MLP,\n",
        "we need to specify the architecture separately.\n",
        "The reason for this is that the models themselves can contain arbitrary code,\n",
        "hence they cannot be serialized as naturally.\n",
        "Thus, in order to reinstate a model, we need\n",
        "to generate the architecture in code\n",
        "and then load the parameters from disk.\n",
        "(**Let's start with our familiar MLP.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6917c6ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.290900Z",
          "iopub.status.busy": "2023-08-18T20:14:04.290420Z",
          "iopub.status.idle": "2023-08-18T20:14:04.301461Z",
          "shell.execute_reply": "2023-08-18T20:14:04.300490Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "6917c6ad"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.LazyLinear(256)\n",
        "        self.layer2 = nn.LazyLinear(100)\n",
        "        self.output = nn.LazyLinear(10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.output(x)\n",
        "\n",
        "net = MLP()\n",
        "X = torch.randn(size=(2, 20))\n",
        "Y = net(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d0c48e",
      "metadata": {
        "origin_pos": 31,
        "id": "45d0c48e"
      },
      "source": [
        "Next, we [**store the parameters of the model as a file**] with the name \"mlp.params\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "88dfe184",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.304908Z",
          "iopub.status.busy": "2023-08-18T20:14:04.304617Z",
          "iopub.status.idle": "2023-08-18T20:14:04.309701Z",
          "shell.execute_reply": "2023-08-18T20:14:04.308927Z"
        },
        "origin_pos": 33,
        "tab": [
          "pytorch"
        ],
        "id": "88dfe184"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), 'mlp.params')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ddcdcbd",
      "metadata": {
        "origin_pos": 36,
        "id": "3ddcdcbd"
      },
      "source": [
        "To recover the model, we instantiate a clone\n",
        "of the original MLP model.\n",
        "Instead of randomly initializing the model parameters,\n",
        "we [**read the parameters stored in the file directly**].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0a8e9c03",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.312721Z",
          "iopub.status.busy": "2023-08-18T20:14:04.312444Z",
          "iopub.status.idle": "2023-08-18T20:14:04.320044Z",
          "shell.execute_reply": "2023-08-18T20:14:04.319202Z"
        },
        "origin_pos": 38,
        "tab": [
          "pytorch"
        ],
        "id": "0a8e9c03",
        "outputId": "9741d689-a78b-4003-835d-62d5b4822395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "clone = MLP()\n",
        "clone.load_state_dict(torch.load('mlp.params'))\n",
        "clone.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b209774",
      "metadata": {
        "origin_pos": 41,
        "id": "4b209774"
      },
      "source": [
        "Since both instances have the same model parameters,\n",
        "the computational result of the same input `X` should be the same.\n",
        "Let's verify this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d65ae251",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:14:04.323094Z",
          "iopub.status.busy": "2023-08-18T20:14:04.322816Z",
          "iopub.status.idle": "2023-08-18T20:14:04.330451Z",
          "shell.execute_reply": "2023-08-18T20:14:04.329304Z"
        },
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "d65ae251",
        "outputId": "a69f718f-37ff-4d39-8993-c075ab76d820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True, True, True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "Y_clone = clone(X)\n",
        "Y_clone == Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109a1a9f",
      "metadata": {
        "origin_pos": 44,
        "id": "109a1a9f"
      },
      "source": [
        "## Summary\n",
        "\n",
        "The `save` and `load` functions can be used to perform file I/O for tensor objects.\n",
        "We can save and load the entire sets of parameters for a network via a parameter dictionary.\n",
        "Saving the architecture has to be done in code rather than in parameters.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Even if there is no need to deploy trained models to a different device, what are the practical benefits of storing model parameters?\n",
        "1. Assume that we want to reuse only parts of a network to be incorporated into a network having a different architecture. How would you go about using, say the first two layers from a previous network in a new network?\n",
        "1. How would you go about saving the network architecture and parameters? What restrictions would you impose on the architecture?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815c6198",
      "metadata": {
        "origin_pos": 46,
        "tab": [
          "pytorch"
        ],
        "id": "815c6198"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/61)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/\n",
        "- In the definitions of checkpoints and fault tolerance, training will consume hours and days, if the program crashs, electricity is off or CPU is consumed all, we can resume training once these are resolved from the checkpoint.\n",
        "- We also save the model parameters at the best epoch of training stage and continue from it when it is appropriate. (Early stopping and Model selection)\n",
        "- Reproducibility: saving the model parameters can help us visualize the result, debug, compare results.\n",
        "- It also helps us save many seeds of different hyperparameters that we can evaluate offline.\n",
        "- The data (model parameters) will be persistent cause on RAM/CPU these are temporary and we can free up CPU usage for other purposes.\n"
      ],
      "metadata": {
        "id": "8_3Tg9tJez1S"
      },
      "id": "8_3Tg9tJez1S"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.We can save the learned parameters of these two first layers using state_dict, then load those parametes into corresponding layers for new network.\n",
        "class NewMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.LazyLinear(256)\n",
        "        self.layer2 = nn.LazyLinear(100)\n",
        "        self.output = nn.LazyLinear(5) # different architecture\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.output(x)\n",
        "\n",
        "net.load_state_dict(torch.load('mlp.params'))\n",
        "new_net = NewMLP()\n",
        "\n",
        "# Materialize lazy layers\n",
        "dummy = torch.rand(1, 20)\n",
        "_ = net(dummy)\n",
        "_ = new_net(dummy)\n",
        "\n",
        "new_net.layer1.load_state_dict(net.layer1.state_dict())\n",
        "new_net.layer2.load_state_dict(net.layer2.state_dict())\n",
        "\n",
        "# Freeze the copied layers\n",
        "for param in new_net.layer1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in new_net.layer2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "X = torch.randn(size=(3, 20))\n",
        "Y = new_net(X)\n"
      ],
      "metadata": {
        "id": "rr4VsZLfgVi3"
      },
      "id": "rr4VsZLfgVi3",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "i1QB9VAsgf6e",
        "outputId": "8dd37aa3-761e-475c-d556-6728583935d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i1QB9VAsgf6e",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('hidden.weight',\n",
              "              tensor([[ 0.1514,  0.0836, -0.0181,  ..., -0.1981, -0.1966,  0.2334],\n",
              "                      [ 0.1261, -0.0033,  0.0632,  ..., -0.0187,  0.0513,  0.1465],\n",
              "                      [-0.1790,  0.0706,  0.1552,  ..., -0.1704,  0.0469, -0.1167],\n",
              "                      ...,\n",
              "                      [ 0.2271, -0.1124, -0.0528,  ..., -0.0657, -0.1134, -0.2157],\n",
              "                      [-0.0656,  0.2199, -0.1157,  ...,  0.2156,  0.1562,  0.0876],\n",
              "                      [ 0.1245,  0.0159, -0.1745,  ..., -0.0892, -0.0395,  0.2128]])),\n",
              "             ('hidden.bias',\n",
              "              tensor([-0.1660,  0.0425, -0.2298,  0.2185, -0.0333,  0.1166,  0.1079, -0.0093,\n",
              "                      -0.0383,  0.2181, -0.1474, -0.1324, -0.1836, -0.0031,  0.2067, -0.1114,\n",
              "                      -0.0347, -0.1826, -0.1885,  0.0362,  0.2185,  0.0708,  0.2284,  0.1400,\n",
              "                      -0.0915, -0.2036,  0.1653, -0.0253, -0.1238, -0.1081, -0.1122,  0.0230,\n",
              "                      -0.0150, -0.1597,  0.1416,  0.0900,  0.0903, -0.0881, -0.1057, -0.0532,\n",
              "                       0.1698,  0.1289, -0.0977, -0.1881,  0.0966,  0.1405,  0.1079, -0.0814,\n",
              "                       0.1494, -0.2125, -0.1887, -0.0496, -0.1459,  0.2027,  0.0547, -0.0653,\n",
              "                       0.0170, -0.0597, -0.1037, -0.0233,  0.1936, -0.0829,  0.2337, -0.0485,\n",
              "                      -0.1620,  0.2055, -0.0985, -0.2231, -0.1075, -0.0005, -0.0829, -0.1840,\n",
              "                      -0.0958,  0.1377, -0.0671,  0.1127, -0.2156, -0.2220, -0.0831, -0.1281,\n",
              "                       0.0230,  0.0613, -0.1690,  0.0766,  0.0208,  0.0337, -0.1798,  0.2000,\n",
              "                      -0.2324, -0.1121, -0.1156, -0.1701, -0.1854, -0.1346,  0.0301,  0.1295,\n",
              "                       0.1010,  0.1257,  0.0273, -0.2195, -0.1784, -0.0699,  0.1820, -0.0457,\n",
              "                       0.0935, -0.2145, -0.0743, -0.0020, -0.0815, -0.0680, -0.1738,  0.2221,\n",
              "                       0.0558,  0.0857, -0.0712,  0.1087, -0.2251, -0.0246, -0.1980, -0.1440,\n",
              "                       0.2081,  0.2126,  0.1961,  0.0280, -0.0092,  0.1120, -0.0884, -0.0702,\n",
              "                       0.1869,  0.0828,  0.0611,  0.2319,  0.2187, -0.0374,  0.0527, -0.1220,\n",
              "                      -0.1158, -0.0956,  0.0830, -0.0017,  0.0374, -0.0764, -0.0584,  0.0807,\n",
              "                       0.1418,  0.2032,  0.0409, -0.1659, -0.0104,  0.2355,  0.1517,  0.2328,\n",
              "                      -0.1051,  0.1514, -0.1634,  0.2344,  0.0144,  0.1739, -0.1518, -0.2156,\n",
              "                       0.1175,  0.0241,  0.1342, -0.1540, -0.2217,  0.0984, -0.1344, -0.1888,\n",
              "                       0.0517, -0.1783, -0.1394,  0.1986,  0.1859, -0.0291,  0.2212,  0.0653,\n",
              "                      -0.1851,  0.0432, -0.0037, -0.0223,  0.0849,  0.0320,  0.0250, -0.1707,\n",
              "                      -0.0437,  0.1532, -0.0171, -0.0166,  0.1575, -0.0512, -0.0648,  0.0033,\n",
              "                       0.1749, -0.0753, -0.0067,  0.2193, -0.1517,  0.1202,  0.1452, -0.2307,\n",
              "                      -0.1496,  0.0488, -0.1501, -0.1781,  0.1431,  0.1525,  0.2319, -0.1407,\n",
              "                      -0.2009, -0.1204,  0.0340, -0.1448, -0.2025, -0.1660,  0.1581, -0.0273,\n",
              "                       0.0147,  0.1688, -0.2046, -0.0332, -0.1732,  0.0910, -0.1364,  0.0179,\n",
              "                      -0.1089, -0.0190, -0.2223,  0.0199,  0.1551,  0.0773,  0.1635,  0.0679,\n",
              "                      -0.0116,  0.0389, -0.1427,  0.2156,  0.0502,  0.0494,  0.1585, -0.1195,\n",
              "                      -0.2075,  0.2091, -0.0303,  0.1707, -0.1349,  0.1191, -0.1484,  0.1738,\n",
              "                       0.0941,  0.0126,  0.1060, -0.1394,  0.0230,  0.0372, -0.1078, -0.0925])),\n",
              "             ('output.weight',\n",
              "              tensor([[ 0.0569, -0.0092, -0.0332,  ..., -0.0060, -0.0205,  0.0474],\n",
              "                      [-0.0431, -0.0081, -0.0130,  ...,  0.0445,  0.0035,  0.0001],\n",
              "                      [-0.0044, -0.0131,  0.0598,  ..., -0.0266,  0.0025, -0.0143],\n",
              "                      ...,\n",
              "                      [-0.0403,  0.0347,  0.0537,  ...,  0.0250,  0.0051,  0.0490],\n",
              "                      [-0.0006,  0.0270,  0.0583,  ..., -0.0170, -0.0486, -0.0525],\n",
              "                      [ 0.0140, -0.0414,  0.0356,  ...,  0.0410, -0.0419, -0.0201]])),\n",
              "             ('output.bias',\n",
              "              tensor([ 0.0155,  0.0534,  0.0400, -0.0539,  0.0420,  0.0411, -0.0097,  0.0454,\n",
              "                      -0.0467, -0.0376]))])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/ If we save all the model, like: torch.save(model, \"model.pkl\") then it will depend very closely on code, Pytorch version, hard to maintain and to be reusable, bugs when refactor. Occasionally, we often save model parameters or configurations of models (in JSON or YAML format) and load it more efficiently."
      ],
      "metadata": {
        "id": "kKcgXWOtkx-0"
      },
      "id": "kKcgXWOtkx-0"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wS1sdlv1lPey"
      },
      "id": "wS1sdlv1lPey",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}