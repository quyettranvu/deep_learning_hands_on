{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f678e9c",
      "metadata": {
        "id": "9f678e9c"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e92ec675",
      "metadata": {
        "id": "e92ec675",
        "outputId": "bb219149-08eb-45a5-bc8b-3c880aa0c463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Collecting d2l==1.0.3\n",
            "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
            "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-1.0.3\n",
            "Collecting numpy<2,>=1.26\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter)\n",
            "  Downloading jupyterlab-4.5.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (5.9.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->jupyter) (4.5.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter) (3.0.16)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter) (1.3.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->jupyter) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->jupyter) (4.25.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter) (0.30.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (2.14.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (4.12.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (3.11)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (4.15.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter) (25.1.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.23)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.8)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter) (1.4.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter) (2.5.0)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.5.1-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m169.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: numpy, json5, jedi, async-lru, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [jupyter]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "d2l 1.0.3 requires jupyter==1.0.0, but you have jupyter 1.1.1 which is incompatible.\n",
            "d2l 1.0.3 requires matplotlib==3.7.2, but you have matplotlib 3.10.0 which is incompatible.\n",
            "d2l 1.0.3 requires matplotlib-inline==0.1.6, but you have matplotlib-inline 0.2.1 which is incompatible.\n",
            "d2l 1.0.3 requires numpy==1.23.5, but you have numpy 1.26.4 which is incompatible.\n",
            "d2l 1.0.3 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\n",
            "d2l 1.0.3 requires requests==2.31.0, but you have requests 2.32.4 which is incompatible.\n",
            "d2l 1.0.3 requires scipy==1.10.1, but you have scipy 1.16.3 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed async-lru-2.0.5 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.5.1 jupyterlab-server-2.28.0 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9b64b1bede2a41a0999daf682632d023"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "d2l OK, numpy: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# keep pip up to date\n",
        "%pip install -U pip\n",
        "\n",
        "# install d2l but skip its (too strict) dependencies\n",
        "%pip install d2l==1.0.3 --no-deps\n",
        "\n",
        "# install dependencies compatible with Python 3.12\n",
        "# NumPy >= 1.26 has Py3.12 wheels\n",
        "%pip install \"numpy>=1.26,<2\" matplotlib pandas jupyter\n",
        "\n",
        "# Choose the right index for your runtime (CPU vs CUDA). Example for CUDA 12.4:\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "# Or CPU-only:\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "import d2l, numpy as np\n",
        "print(\"d2l OK, numpy:\", np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25fee3b7",
      "metadata": {
        "origin_pos": 1,
        "id": "25fee3b7"
      },
      "source": [
        "# GPUs\n",
        ":label:`sec_use_gpu`\n",
        "\n",
        "In :numref:`tab_intro_decade`, we illustrated the rapid growth\n",
        "of computation over the past two decades.\n",
        "In a nutshell, GPU performance has increased\n",
        "by a factor of 1000 every decade since 2000.\n",
        "This offers great opportunities but it also suggests\n",
        "that there was significant demand for such performance.\n",
        "\n",
        "\n",
        "In this section, we begin to discuss how to harness\n",
        "this computational performance for your research.\n",
        "First by using a single GPU and at a later point,\n",
        "how to use multiple GPUs and multiple servers (with multiple GPUs).\n",
        "\n",
        "Specifically, we will discuss how\n",
        "to use a single NVIDIA GPU for calculations.\n",
        "First, make sure you have at least one NVIDIA GPU installed.\n",
        "Then, download the [NVIDIA driver and CUDA](https://developer.nvidia.com/cuda-downloads)\n",
        "and follow the prompts to set the appropriate path.\n",
        "Once these preparations are complete,\n",
        "the `nvidia-smi` command can be used\n",
        "to (**view the graphics card information**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92058ee6",
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "92058ee6"
      },
      "source": [
        "In PyTorch, every array has a device; we often refer it as a *context*.\n",
        "So far, by default, all variables\n",
        "and associated computation\n",
        "have been assigned to the CPU.\n",
        "Typically, other contexts might be various GPUs.\n",
        "Things can get even hairier when\n",
        "we deploy jobs across multiple servers.\n",
        "By assigning arrays to contexts intelligently,\n",
        "we can minimize the time spent\n",
        "transferring data between devices.\n",
        "For example, when training neural networks on a server with a GPU,\n",
        "we typically prefer for the model's parameters to live on the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49bda574",
      "metadata": {
        "origin_pos": 4,
        "id": "49bda574"
      },
      "source": [
        "To run the programs in this section,\n",
        "you need at least two GPUs.\n",
        "Note that this might be extravagant for most desktop computers\n",
        "but it is easily available in the cloud, e.g.,\n",
        "by using the AWS EC2 multi-GPU instances.\n",
        "Almost all other sections do *not* require multiple GPUs, but here we simply wish to illustrate data flow between different devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a4ef6e6b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.270913Z",
          "iopub.status.busy": "2023-08-18T19:36:58.270055Z",
          "iopub.status.idle": "2023-08-18T19:37:01.897059Z",
          "shell.execute_reply": "2023-08-18T19:37:01.896067Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "a4ef6e6b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0006dfe3",
      "metadata": {
        "origin_pos": 9,
        "id": "0006dfe3"
      },
      "source": [
        "## [**Computing Devices**]\n",
        "\n",
        "We can specify devices, such as CPUs and GPUs,\n",
        "for storage and calculation.\n",
        "By default, tensors are created in the main memory\n",
        "and then the CPU is used for calculations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff8c64e",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "dff8c64e"
      },
      "source": [
        "In PyTorch, the CPU and GPU can be indicated by `torch.device('cpu')` and `torch.device('cuda')`.\n",
        "It should be noted that the `cpu` device\n",
        "means all physical CPUs and memory.\n",
        "This means that PyTorch's calculations\n",
        "will try to use all CPU cores.\n",
        "However, a `gpu` device only represents one card\n",
        "and the corresponding memory.\n",
        "If there are multiple GPUs, we use `torch.device(f'cuda:{i}')`\n",
        "to represent the $i^\\textrm{th}$ GPU ($i$ starts at 0).\n",
        "Also, `gpu:0` and `gpu` are equivalent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d996a07b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:01.901957Z",
          "iopub.status.busy": "2023-08-18T19:37:01.901006Z",
          "iopub.status.idle": "2023-08-18T19:37:01.911076Z",
          "shell.execute_reply": "2023-08-18T19:37:01.909836Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "d996a07b",
        "outputId": "f69d7f09-1a7a-458c-cbb3-69685d775d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cpu'),\n",
              " device(type='cuda', index=0),\n",
              " device(type='cuda', index=1))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "def cpu():  #@save\n",
        "    \"\"\"Get the CPU device.\"\"\"\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def gpu(i=0):  #@save\n",
        "    \"\"\"Get a GPU device.\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "\n",
        "cpu(), gpu(), gpu(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a643379",
      "metadata": {
        "origin_pos": 14,
        "id": "0a643379"
      },
      "source": [
        "We can (**query the number of available GPUs.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b20d4266",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:01.915209Z",
          "iopub.status.busy": "2023-08-18T19:37:01.914386Z",
          "iopub.status.idle": "2023-08-18T19:37:01.922363Z",
          "shell.execute_reply": "2023-08-18T19:37:01.921100Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "b20d4266",
        "outputId": "dea222db-a954-49b8-b5f7-b112d5b3b4f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def num_gpus():  #@save\n",
        "    \"\"\"Get the number of available GPUs.\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "\n",
        "num_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab10cfc5",
      "metadata": {
        "origin_pos": 17,
        "id": "ab10cfc5"
      },
      "source": [
        "Now we [**define two convenient functions that allow us\n",
        "to run code even if the requested GPUs do not exist.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ac547f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:01.926431Z",
          "iopub.status.busy": "2023-08-18T19:37:01.925574Z",
          "iopub.status.idle": "2023-08-18T19:37:01.935019Z",
          "shell.execute_reply": "2023-08-18T19:37:01.933960Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "6ac547f6",
        "outputId": "d03d9c4f-c232-4791-fd29-d67957bc9320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0),\n",
              " device(type='cpu'),\n",
              " [device(type='cuda', index=0)])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def try_gpu(i=0):  #@save\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if num_gpus() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "def try_all_gpus():  #@save\n",
        "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
        "    return [gpu(i) for i in range(num_gpus())]\n",
        "\n",
        "try_gpu(), try_gpu(10), try_all_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d23836",
      "metadata": {
        "origin_pos": 19,
        "id": "73d23836"
      },
      "source": [
        "## Tensors and GPUs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04367f7",
      "metadata": {
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "b04367f7"
      },
      "source": [
        "By default, tensors are created on the CPU.\n",
        "We can [**query the device where the tensor is located.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a3e90ced",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:01.939959Z",
          "iopub.status.busy": "2023-08-18T19:37:01.938949Z",
          "iopub.status.idle": "2023-08-18T19:37:01.950067Z",
          "shell.execute_reply": "2023-08-18T19:37:01.949195Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "a3e90ced",
        "outputId": "e389e296-db55-404d-e26f-4c9d5fc7186f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "x.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c538315e",
      "metadata": {
        "origin_pos": 27,
        "id": "c538315e"
      },
      "source": [
        "It is important to note that whenever we want\n",
        "to operate on multiple terms,\n",
        "they need to be on the same device.\n",
        "For instance, if we sum two tensors,\n",
        "we need to make sure that both arguments\n",
        "live on the same device---otherwise the framework\n",
        "would not know where to store the result\n",
        "or even how to decide where to perform the computation.\n",
        "\n",
        "### Storage on the GPU\n",
        "\n",
        "There are several ways to [**store a tensor on the GPU.**]\n",
        "For example, we can specify a storage device when creating a tensor.\n",
        "Next, we create the tensor variable `X` on the first `gpu`.\n",
        "The tensor created on a GPU only consumes the memory of this GPU.\n",
        "We can use the `nvidia-smi` command to view GPU memory usage.\n",
        "In general, we need to make sure that we do not create data that exceeds the GPU memory limit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13913886",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:01.953772Z",
          "iopub.status.busy": "2023-08-18T19:37:01.953191Z",
          "iopub.status.idle": "2023-08-18T19:37:02.420258Z",
          "shell.execute_reply": "2023-08-18T19:37:02.419290Z"
        },
        "origin_pos": 29,
        "tab": [
          "pytorch"
        ],
        "id": "13913886",
        "outputId": "beb0ecd0-109b-42bd-ae5e-48f1fd6fa9ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X = torch.ones(2, 3, device=try_gpu())\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ea65fc",
      "metadata": {
        "origin_pos": 32,
        "id": "32ea65fc"
      },
      "source": [
        "Assuming that you have at least two GPUs, the following code will (**create a random tensor, `Y`, on the second GPU.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f4c7aff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.424924Z",
          "iopub.status.busy": "2023-08-18T19:37:02.424008Z",
          "iopub.status.idle": "2023-08-18T19:37:02.688334Z",
          "shell.execute_reply": "2023-08-18T19:37:02.687371Z"
        },
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ],
        "id": "6f4c7aff",
        "outputId": "5a3469c2-857b-4626-af12-2f61580f9e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3855, 0.9210, 0.9824],\n",
              "        [0.5975, 0.6907, 0.1974]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "Y = torch.rand(2, 3, device=try_gpu(1))\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3bf740b",
      "metadata": {
        "origin_pos": 37,
        "id": "f3bf740b"
      },
      "source": [
        "### Copying\n",
        "\n",
        "[**If we want to compute `X + Y`,\n",
        "we need to decide where to perform this operation.**]\n",
        "For instance, as shown in :numref:`fig_copyto`,\n",
        "we can transfer `X` to the second GPU\n",
        "and perform the operation there.\n",
        "*Do not* simply add `X` and `Y`,\n",
        "since this will result in an exception.\n",
        "The runtime engine would not know what to do:\n",
        "it cannot find data on the same device and it fails.\n",
        "Since `Y` lives on the second GPU,\n",
        "we need to move `X` there before we can add the two.\n",
        "\n",
        "![Copy data to perform an operation on the same device.](http://d2l.ai/_images/copyto.svg)\n",
        ":label:`fig_copyto`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3560f0b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.693634Z",
          "iopub.status.busy": "2023-08-18T19:37:02.693201Z",
          "iopub.status.idle": "2023-08-18T19:37:02.701839Z",
          "shell.execute_reply": "2023-08-18T19:37:02.701004Z"
        },
        "origin_pos": 39,
        "tab": [
          "pytorch"
        ],
        "id": "3560f0b5",
        "outputId": "4bed2fd6-4b3e-49be-d110-0bbe8c411f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1839941265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "Z = X.cuda(1)\n",
        "print(X)\n",
        "print(Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc2252c",
      "metadata": {
        "origin_pos": 42,
        "id": "5cc2252c"
      },
      "source": [
        "Now that [**the data (both `Z` and `Y`) are on the same GPU), we can add them up.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2cfea6e5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.707070Z",
          "iopub.status.busy": "2023-08-18T19:37:02.705679Z",
          "iopub.status.idle": "2023-08-18T19:37:02.735588Z",
          "shell.execute_reply": "2023-08-18T19:37:02.734193Z"
        },
        "origin_pos": 43,
        "tab": [
          "pytorch"
        ],
        "id": "2cfea6e5",
        "outputId": "b25ef757-3a5b-4980-bbb9-1289b63959fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Z' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2311139669.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
          ]
        }
      ],
      "source": [
        "Y + Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4657c339",
      "metadata": {
        "origin_pos": 45,
        "tab": [
          "pytorch"
        ],
        "id": "4657c339"
      },
      "source": [
        "But what if your variable `Z` already lived on your second GPU?\n",
        "What happens if we still call `Z.cuda(1)`?\n",
        "It will return `Z` instead of making a copy and allocating new memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0450cb7c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.743585Z",
          "iopub.status.busy": "2023-08-18T19:37:02.743275Z",
          "iopub.status.idle": "2023-08-18T19:37:02.750645Z",
          "shell.execute_reply": "2023-08-18T19:37:02.748215Z"
        },
        "origin_pos": 49,
        "tab": [
          "pytorch"
        ],
        "id": "0450cb7c",
        "outputId": "2fb497a0-04a7-4c6e-ca6c-474e6f0dc0a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z.cuda(1) is Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5658048c",
      "metadata": {
        "origin_pos": 52,
        "id": "5658048c"
      },
      "source": [
        "### Side Notes\n",
        "\n",
        "People use GPUs to do machine learning\n",
        "because they expect them to be fast.\n",
        "But transferring variables between devices is slow: much slower than computation.\n",
        "So we want you to be 100% certain\n",
        "that you want to do something slow before we let you do it.\n",
        "If the deep learning framework just did the copy automatically\n",
        "without crashing then you might not realize\n",
        "that you had written some slow code.\n",
        "\n",
        "Transferring data is not only slow, it also makes parallelization a lot more difficult,\n",
        "since we have to wait for data to be sent (or rather to be received)\n",
        "before we can proceed with more operations.\n",
        "This is why copy operations should be taken with great care.\n",
        "As a rule of thumb, many small operations\n",
        "are much worse than one big operation.\n",
        "Moreover, several operations at a time\n",
        "are much better than many single operations interspersed in the code\n",
        "unless you know what you are doing.\n",
        "This is the case since such operations can block if one device\n",
        "has to wait for the other before it can do something else.\n",
        "It is a bit like ordering your coffee in a queue\n",
        "rather than pre-ordering it by phone\n",
        "and finding out that it is ready when you are.\n",
        "\n",
        "Last, when we print tensors or convert tensors to the NumPy format,\n",
        "if the data is not in the main memory,\n",
        "the framework will copy it to the main memory first,\n",
        "resulting in additional transmission overhead.\n",
        "Even worse, it is now subject to the dreaded global interpreter lock\n",
        "that makes everything wait for Python to complete.\n",
        "\n",
        "\n",
        "## [**Neural Networks and GPUs**]\n",
        "\n",
        "Similarly, a neural network model can specify devices.\n",
        "The following code puts the model parameters on the GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8bcc281a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.756785Z",
          "iopub.status.busy": "2023-08-18T19:37:02.756022Z",
          "iopub.status.idle": "2023-08-18T19:37:02.763247Z",
          "shell.execute_reply": "2023-08-18T19:37:02.762013Z"
        },
        "origin_pos": 54,
        "tab": [
          "pytorch"
        ],
        "id": "8bcc281a"
      },
      "outputs": [],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(1))\n",
        "net = net.to(device=try_gpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb2c254",
      "metadata": {
        "origin_pos": 57,
        "id": "4fb2c254"
      },
      "source": [
        "We will see many more examples of\n",
        "how to run models on GPUs in the following chapters,\n",
        "simply because the models will become somewhat more computationally intensive.\n",
        "\n",
        "For example, when the input is a tensor on the GPU, the model will calculate the result on the same GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "351af69d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.768539Z",
          "iopub.status.busy": "2023-08-18T19:37:02.767413Z",
          "iopub.status.idle": "2023-08-18T19:37:02.809950Z",
          "shell.execute_reply": "2023-08-18T19:37:02.807298Z"
        },
        "origin_pos": 58,
        "tab": [
          "pytorch"
        ],
        "id": "351af69d",
        "outputId": "6c39938c-536e-4209-88f0-0b8e7310e6ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3610],\n",
              "        [0.3610]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "net(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad9b55b",
      "metadata": {
        "origin_pos": 60,
        "id": "1ad9b55b"
      },
      "source": [
        "Let's (**confirm that the model parameters are stored on the same GPU.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6fdbd2c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.816317Z",
          "iopub.status.busy": "2023-08-18T19:37:02.815749Z",
          "iopub.status.idle": "2023-08-18T19:37:02.822467Z",
          "shell.execute_reply": "2023-08-18T19:37:02.821657Z"
        },
        "origin_pos": 62,
        "tab": [
          "pytorch"
        ],
        "id": "6fdbd2c3",
        "outputId": "01af66bd-0160-4598-e36d-3053e03b73b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "net[0].weight.data.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb5940ac",
      "metadata": {
        "origin_pos": 65,
        "id": "eb5940ac"
      },
      "source": [
        "Let the trainer support GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1283ae3a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:37:02.826029Z",
          "iopub.status.busy": "2023-08-18T19:37:02.825482Z",
          "iopub.status.idle": "2023-08-18T19:37:02.832065Z",
          "shell.execute_reply": "2023-08-18T19:37:02.831156Z"
        },
        "origin_pos": 67,
        "tab": [
          "pytorch"
        ],
        "id": "1283ae3a"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "    self.save_hyperparameters()\n",
        "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def prepare_batch(self, batch):\n",
        "    if self.gpus:\n",
        "        batch = [a.to(self.gpus[0]) for a in batch]\n",
        "    return batch\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def prepare_model(self, model):\n",
        "    model.trainer = self\n",
        "    model.board.xlim = [0, self.max_epochs]\n",
        "    if self.gpus:\n",
        "        model.to(self.gpus[0])\n",
        "    self.model = model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f33c768",
      "metadata": {
        "origin_pos": 69,
        "id": "4f33c768"
      },
      "source": [
        "In short, as long as all data and parameters are on the same device, we can learn models efficiently. In the following chapters we will see several such examples.\n",
        "\n",
        "## Summary\n",
        "\n",
        "We can specify devices for storage and calculation, such as the CPU or GPU.\n",
        "  By default, data is created in the main memory\n",
        "  and then uses the CPU for calculations.\n",
        "The deep learning framework requires all input data for calculation\n",
        "  to be on the same device,\n",
        "  be it CPU or the same GPU.\n",
        "You can lose significant performance by moving data without care.\n",
        "  A typical mistake is as follows: computing the loss\n",
        "  for every minibatch on the GPU and reporting it back\n",
        "  to the user on the command line (or logging it in a NumPy `ndarray`)\n",
        "  will trigger a global interpreter lock which stalls all GPUs.\n",
        "  It is much better to allocate memory\n",
        "  for logging inside the GPU and only move larger logs.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Try a larger computation task, such as the multiplication of large matrices,\n",
        "   and see the difference in speed between the CPU and GPU.\n",
        "   What about a task with a small number of calculations?\n",
        "1. How should we read and write model parameters on the GPU?\n",
        "1. Measure the time it takes to compute 1000\n",
        "   matrix--matrix multiplications of $100 \\times 100$ matrices\n",
        "   and log the Frobenius norm of the output matrix one result at a time. Compare it with keeping a log on the GPU and transferring only the final result.\n",
        "1. Measure how much time it takes to perform two matrix--matrix multiplications\n",
        "   on two GPUs at the same time. Compare it with computing in in sequence\n",
        "   on one GPU. Hint: you should see almost linear scaling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3cfc42b",
      "metadata": {
        "origin_pos": 71,
        "tab": [
          "pytorch"
        ],
        "id": "b3cfc42b"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/63)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "Z = torch.rand(100, 10000)\n",
        "X = torch.rand(100, 10000)\n",
        "\n",
        "start = time.time()\n",
        "Y = Z * X\n",
        "end = time.time()\n",
        "\n",
        "print(f\"CPU Time: {end - start:.6f} seconds\")"
      ],
      "metadata": {
        "id": "264kidRf_gFR",
        "outputId": "2c2e8844-6bb2-4f1b-e1eb-e95312bdb943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "264kidRf_gFR",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Time: 0.003137 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "Z = torch.rand(100, 10000, device=device)\n",
        "X = torch.rand(100, 10000, device=device)\n",
        "\n",
        "torch.cuda.synchronize() # ensure GPU idle before timing\n",
        "start = time.time()\n",
        "Y = Z * X\n",
        "\n",
        "torch.cuda.synchronize() # ensure GPU operations finished before returning control to CPU\n",
        "end = time.time()\n",
        "\n",
        "print(f\"GPU Time: {end - start:.6f} seconds\")"
      ],
      "metadata": {
        "id": "a5Nmvp97AF8j",
        "outputId": "07cf3e13-0814-4a9a-d76d-4eff2d622b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a5Nmvp97AF8j",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Time: 0.000215 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With small number of calculations, we do not benefit from using GPU compared to using CPU for calculations."
      ],
      "metadata": {
        "id": "kGmN329XBc6F"
      },
      "id": "kGmN329XBc6F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2/ Model parameters should be saved and loaded using state_dict which serializes via CPU and disk. After loading from disk, the data can be used for calculations on GPU, direct loading is not supported because GPU memory is volatile, data loss when process finishes."
      ],
      "metadata": {
        "id": "y71uS-kNBnb4"
      },
      "id": "y71uS-kNBnb4"
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones(2, 4, device=try_gpu())\n",
        "Y = torch.ones(3, 4) # on GPU\n",
        "\n",
        "net = nn.Sequential(nn.LazyLinear(2)) # materialize : 2 out features\n",
        "net = net.to(device=try_gpu())\n",
        "net(X)\n",
        "\n",
        "new_net = nn.Sequential(nn.LazyLinear(2)) # materialize: 2 out features\n",
        "new_net(Y)\n",
        "\n",
        "torch.save(new_net.state_dict(), \"model_state.pt\")\n"
      ],
      "metadata": {
        "id": "ITiJ-cdzC4hI"
      },
      "id": "ITiJ-cdzC4hI",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net(X)"
      ],
      "metadata": {
        "id": "guGwur49DbCw",
        "outputId": "7464b11a-5a54-4217-9692-bcd2305a561e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "guGwur49DbCw",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7517,  0.0565],\n",
              "        [-0.7517,  0.0565]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_net(Y)"
      ],
      "metadata": {
        "id": "yD6RfhbbDhem",
        "outputId": "982537c6-9b52-4072-ff38-d57e5113eba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yD6RfhbbDhem",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4990, 0.1967],\n",
              "        [0.4990, 0.1967],\n",
              "        [0.4990, 0.1967]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.load_state_dict(torch.load(\"model_state.pt\"))\n",
        "net.to(device=\"cuda\")"
      ],
      "metadata": {
        "id": "RiTpXcSUDkjZ",
        "outputId": "db833fa3-1f0b-4a9b-b9ab-b7b6c917a0a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RiTpXcSUDkjZ",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=4, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3\n",
        "A = torch.rand(100, 100, device = 'cuda')\n",
        "B = torch.rand(100, 100, device = 'cuda')\n",
        "log = []\n",
        "\n",
        "start = time.time()\n",
        "for i in range(1000):\n",
        "  C = A @ B\n",
        "  log.append(torch.norm(C, p = 'fro').item())\n",
        "\n",
        "end = time.time()\n",
        "print(f\"CPU Time: {end - start:.6f} seconds\")"
      ],
      "metadata": {
        "id": "etMgGU1HIOgP",
        "outputId": "8997adf4-e872-4c91-a724-62507a6e6493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "etMgGU1HIOgP",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Time: 0.060848 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs = torch.empty(1000, device = 'cuda')\n",
        "\n",
        "start = time.time()\n",
        "for i in range(1000):\n",
        "  C = A @ B\n",
        "  logs[i] = torch.norm(C, p = 'fro') # on GPU\n",
        "\n",
        "final = logs.cpu()\n",
        "end = time.time()\n",
        "print(f\"GPU Time: {end - start:.6f} seconds\")"
      ],
      "metadata": {
        "id": "BT1tx8-NI1mD",
        "outputId": "442eb5a7-7766-4a99-cf8d-813ed2fa0e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BT1tx8-NI1mD",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Time: 0.059095 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4\n",
        "X = torch.rand(3, 2, device=try_gpu())\n",
        "Y = torch.rand(2, 3, device=try_gpu())\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "Z = torch.matmul(X, Y)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "end = time.time()\n",
        "print(f\"GPU Time on same GPU: {end - start:.6f} seconds\")"
      ],
      "metadata": {
        "id": "ze5arEtvJllI",
        "outputId": "3221245a-ca96-479a-c904-8f0a8fb7e909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ze5arEtvJllI",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Time on same GPU: 0.000873 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(3, 2, device=try_gpu())\n",
        "Y = torch.rand(2, 3, device=try_gpu(1))\n",
        "Z = X.cuda(1)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "Z = torch.matmul(X, Y)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "end = time.time()\n",
        "print(f\"GPU Time on different GPU: {end - start:.6f} seconds\") # Calculation will be slower because of copying tensor, sometimes overhead of inter-device data transfer and synchronization"
      ],
      "metadata": {
        "id": "6lRVtxPHKD9L",
        "outputId": "f0a00f22-30e5-4641-dcef-8f5d4f6d217b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "id": "6lRVtxPHKD9L",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-538796627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: invalid device ordinal\nGPU device may be out of range, do you have enough GPUs?\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}